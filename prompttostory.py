{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"id\":\"en8zxAf2XQz6\"}\n# **Downloading Required Libraries**\n\n# %% [code] {\"id\":\"UvuSZnhqHhEf\",\"outputId\":\"db914862-3911-46fc-a307-7116426570fa\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:57:47.294108Z\",\"iopub.execute_input\":\"2022-11-16T12:57:47.294615Z\",\"iopub.status.idle\":\"2022-11-16T12:58:09.315281Z\",\"shell.execute_reply.started\":\"2022-11-16T12:57:47.294514Z\",\"shell.execute_reply\":\"2022-11-16T12:58:09.314121Z\"}}\n!pip install rake-nltk\n!pip install transformers\n!pip install gensim==3.4.0 \n\n# %% [markdown] {\"id\":\"Q5GgrkRpXXzT\"}\n# **Importing Required Libraries**\n\n# %% [code] {\"id\":\"LyQGMdKZB8SJ\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:58:09.317961Z\",\"iopub.execute_input\":\"2022-11-16T12:58:09.318409Z\",\"iopub.status.idle\":\"2022-11-16T12:58:18.918040Z\",\"shell.execute_reply.started\":\"2022-11-16T12:58:09.318366Z\",\"shell.execute_reply\":\"2022-11-16T12:58:18.916820Z\"}}\nimport numpy as np \nimport pandas as pd \nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom rake_nltk import Rake\nimport nltk\nimport re\nfrom transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom gensim.summarization.summarizer import summarize\nfrom ignite.metrics import Rouge\nfrom ignite.metrics.nlp import Bleu\n\n# %% [markdown]\n# New version of Gensim don't have summerize module, so using old version. \n\n# %% [markdown] {\"id\":\"D_f_dbjpXbg2\"}\n# **Downloading Stopward and Punctuations for english vocabulary**\n\n# %% [code] {\"id\":\"JxUx8KL0KGby\",\"outputId\":\"1f834b77-50ea-466a-f517-079daf0ef3f6\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:01.376515Z\",\"iopub.execute_input\":\"2022-11-16T12:59:01.377177Z\",\"iopub.status.idle\":\"2022-11-16T12:59:01.732593Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:01.377107Z\",\"shell.execute_reply\":\"2022-11-16T12:59:01.731530Z\"}}\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# %% [markdown] {\"id\":\"XjLycdI3Xhzp\"}\n# **Loading and preprocessing Data**\n\n# %% [code] {\"id\":\"LXeIyvJWC2Xh\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:01.740789Z\",\"iopub.execute_input\":\"2022-11-16T12:59:01.741556Z\",\"iopub.status.idle\":\"2022-11-16T12:59:01.751995Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:01.741518Z\",\"shell.execute_reply\":\"2022-11-16T12:59:01.750750Z\"}}\n#DIR = \"/content/drive/MyDrive/Story_Generation_ANLP/\"\nDIR=\"/story-generation/Story_Generation_ANLP\"\ndata = [DIR+\"valid\", DIR+\"test\" , DIR+\"train\" ]\ntarget_data = [DIR+\"valid\", DIR+\"test\", DIR+\"train\"]\n\n# %% [markdown] {\"id\":\"eGIKFypBXqq6\"}\n# **Converting Dataset in appropriate Form:**\n# Creating Dataset in appropriate format as mentioned in the paper. Every data instance have an input \"Prompt\" and corresponding story line. Which at this phase is converted to [ WP ] PROMPT <endprompt> OUTLINE <endoutline> TOP 10 KEYWORD/ PHRASE. This is one time step and run at the first iteration. So commenting the code out.\n\n# %% [code] {\"id\":\"iwu8IF-FFrFU\",\"outputId\":\"63462228-5bdc-4a7e-bb2b-83b7d2eebde7\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:01.753363Z\",\"iopub.execute_input\":\"2022-11-16T12:59:01.753847Z\",\"iopub.status.idle\":\"2022-11-16T12:59:01.762867Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:01.753810Z\",\"shell.execute_reply\":\"2022-11-16T12:59:01.761927Z\"}}\n# for path in data:\n#   fp = open(path + \".wp_source\") \n#   ft = open(path + \".wp_target\")\n\n#   stories = ft.readlines()  \n#   prompts = fp.readlines()\n\n#   #rstrip: Remove any white spaces at the end of the string\n#   #we need to get outline from the prompt\n#   i=0\n#   new_stories=[]\n\n#   for i in tqdm(range(len(stories))):\n    \n#     story=stories[i]\n#     prompt=prompts[i]\n\n#     r = Rake()\n#     r.extract_keywords_from_text(story)\n\n#     keywordStory=\"\"\n#     count=0\n\n#     for word in r.get_ranked_phrases():\n#       \"\"\" Ranked phrases return a list of phrases, each phrases can have mutliple words\"\"\"\n#       if count==10: # According to (Yao et al., 2019) and the average lengths of stories in our corpus, we extract 10 keywords for each story.\n#         break\n#       count+=1\n#       keywordStory+=word\n    \n#     storyRemovingNewline=re.sub(\"<newline>\",\"\\n\",story)\n\n#     try:\n#       summary=summarize(storyRemovingNewline,ratio=0.3)\n#       summary=re.sub(\"\\n\",\" \",summary)\n#       new_stories.append(prompt.rstrip() + \" <endprompts> \" + summary.rstrip() +\" <endoutline> \"+keywordStory)\n#     except:\n#       pass\n    \n#   with open(path + \".wp_combined\", \"w\") as o:\n#     for line in new_stories:\n#       o.write(line.strip() + \"\\n\")\n#     print('finish writing',path + \".wp_combined\")\n\n#   fp.close()\n#   ft.close()\n\n# %% [markdown] {\"id\":\"uwAWGWB_uguL\"}\n# When running the session first time, copy the dataset to working directory, as input directory is read only, since kaggle input directory is read only\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:01.764245Z\",\"iopub.execute_input\":\"2022-11-16T12:59:01.764802Z\",\"iopub.status.idle\":\"2022-11-16T12:59:14.575206Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:01.764766Z\",\"shell.execute_reply\":\"2022-11-16T12:59:14.573744Z\"}}\n#!rm -rf /kaggle/working/*\n!cp -r /kaggle/input/ /kaggle/working\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:14.579535Z\",\"iopub.execute_input\":\"2022-11-16T12:59:14.579875Z\",\"iopub.status.idle\":\"2022-11-16T12:59:14.588542Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:14.579842Z\",\"shell.execute_reply\":\"2022-11-16T12:59:14.586219Z\"}}\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# %% [markdown] {\"id\":\"qNpMt8K7YpJl\"}\n# Creating Dataset in appropriate Fromat using Tokenizer and Defining train function for GPT2 finetuning.\n\n# %% [code] {\"id\":\"cGj3kfKcyk4C\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:14.590114Z\",\"iopub.execute_input\":\"2022-11-16T12:59:14.591052Z\",\"iopub.status.idle\":\"2022-11-16T12:59:16.585173Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:14.591003Z\",\"shell.execute_reply\":\"2022-11-16T12:59:16.584034Z\"}}\ndef load_dataset(file_path, tokenizer, block_size = 128):\n    return TextDataset(tokenizer = tokenizer,file_path = file_path,block_size = block_size,)\n\ndef load_data_collator(tokenizer, mlm = False):\n    return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=mlm,)\n    \ndef train(train_file_path,val_file_path,model_name,output_dir,per_device_train_batch_size,per_device_eval_batch_size,num_train_epochs,save_steps):\n  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n  data_collator = load_data_collator(tokenizer)\n  train_dataset = load_dataset(train_file_path, tokenizer)\n  val_dataset = load_dataset(val_file_path,tokenizer)\n    \n  tokenizer.save_pretrained(output_dir)\n      \n  model = GPT2LMHeadModel.from_pretrained(model_name) \n\n  \"\"\"\n    using cross entropy to train on\n    https://github.com/huggingface/transformers/blob/391db836ab7ed2ca61c51a7cf1b135b6ab92be58/transformers/modeling_gpt2.py#L539\n  \"\"\"\n\n  model.save_pretrained(output_dir)\n\n  training_args = TrainingArguments(output_dir=output_dir,\n                                    per_device_train_batch_size=per_device_train_batch_size,\n                                    per_device_eval_batch_size=per_device_eval_batch_size,\n                                    num_train_epochs=num_train_epochs,\n                                    evaluation_strategy=\"steps\",eval_steps=10000,save_strategy=\"no\")\n    \n  trainer = Trainer(model=model,args=training_args,data_collator=data_collator,\n                    train_dataset=train_dataset,eval_dataset=val_dataset)\n      \n  trainer.train()\n  trainer.save_model()\n\n# %% [markdown] {\"id\":\"BV1Hd_8fY10R\"}\n# **Hyperparameters**\n\n# %% [code] {\"id\":\"FbNaoXvMK2G1\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:16.587036Z\",\"iopub.execute_input\":\"2022-11-16T12:59:16.587489Z\",\"iopub.status.idle\":\"2022-11-16T12:59:16.600414Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:16.587447Z\",\"shell.execute_reply\":\"2022-11-16T12:59:16.599450Z\"}}\n# you need to set parameters \npath=\"/kaggle/working/input/story-generation/Story_Generation_ANLP/\"\ntrain_file_path = path+'/train.wp_combined'\nvalidation_file_path= path+'/valid.wp_combined'\nmodel_name = 'gpt2'\noutput_dir = '/kaggle/working/'\nper_device_train_batch_size = 16\nnum_train_epochs = 1 \nsave_steps = 100000\n\n# %% [markdown] {\"id\":\"lXS_-SvsY5nx\"}\n# **Training**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T12:59:55.849537Z\",\"iopub.execute_input\":\"2022-11-16T12:59:55.850785Z\",\"iopub.status.idle\":\"2022-11-16T17:18:27.368190Z\",\"shell.execute_reply.started\":\"2022-11-16T12:59:55.850739Z\",\"shell.execute_reply\":\"2022-11-16T17:18:27.367133Z\"}}\ntrain(\n    train_file_path=train_file_path,\n    val_file_path=validation_file_path,\n    model_name=model_name,\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    per_device_eval_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps\n)\n\n# %% [markdown] {\"id\":\"ESoRxZm4ZQfp\"}\n# **Evaluating Performance of the model**\n\n# %% [code] {\"id\":\"x8k21xt8NZmT\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T18:06:59.638073Z\",\"iopub.execute_input\":\"2022-11-16T18:06:59.638427Z\",\"iopub.status.idle\":\"2022-11-16T18:07:01.437218Z\",\"shell.execute_reply.started\":\"2022-11-16T18:06:59.638396Z\",\"shell.execute_reply\":\"2022-11-16T18:07:01.436217Z\"}}\nmodel_path = '/kaggle/working/'\nFinalModel = GPT2LMHeadModel.from_pretrained(model_path)\nFinalTokenizer = GPT2Tokenizer.from_pretrained(model_path)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T18:21:16.598326Z\",\"iopub.execute_input\":\"2022-11-16T18:21:16.598713Z\",\"iopub.status.idle\":\"2022-11-16T18:21:16.604637Z\",\"shell.execute_reply.started\":\"2022-11-16T18:21:16.598680Z\",\"shell.execute_reply\":\"2022-11-16T18:21:16.603659Z\"}}\ndef inference(sentence,maxlen=500):\n\n    ids = FinalTokenizer.encode(f'{sentence}', return_tensors='pt')\n\n    final_outputs = FinalModel.generate(ids,do_sample=True,max_length=maxlen,\n        pad_token_id=FinalModel.config.eos_token_id,top_k=50,top_p=0.95,)\n\n    return FinalTokenizer.decode(final_outputs[0], skip_special_tokens=True)\n\n# %% [markdown] {\"id\":\"7e46hC4JZHfb\"}\n# **Inference**\n\n# %% [code] {\"id\":\"Y2UB5oFrVfvB\",\"outputId\":\"954c1b89-933e-454b-b0ef-8dff60b32148\",\"execution\":{\"iopub.status.busy\":\"2022-11-16T19:28:18.184219Z\",\"iopub.execute_input\":\"2022-11-16T19:28:18.184769Z\",\"iopub.status.idle\":\"2022-11-16T19:28:18.196301Z\",\"shell.execute_reply.started\":\"2022-11-16T19:28:18.184722Z\",\"shell.execute_reply\":\"2022-11-16T19:28:18.195203Z\"}}\ntestPathS=\"/kaggle/working/input/story-generation/Story_Generation_ANLP/test.wp_source\"\ntestPathT=\"/kaggle/working/input/story-generation/Story_Generation_ANLP/test.wp_target\"\n\nblue = Bleu(ngram=4, smooth=\"smooth1\")\nrougel = Rouge(variants=[\"L\", 2], multiref=\"best\")\nfinal={'Rouge-L-P': 0.0, 'Rouge-L-R': 0.0, 'Rouge-L-F': 0.0, 'Rouge-2-P': 0.0, 'Rouge-2-R': 0.0, 'Rouge-2-F': 0.0}\nfinalBlue=0\ncounter=0\n\ndef evaluateSentence():\n    fileS = open(testPathS)\n    fileT = open(testPathT)\n    sourceLine=fileS.readlines()\n    targetLine=fileT.readlines()\n    finalBlue=0\n    length=0\n    for counter in tqdm(range(100)): \n        sequence=inference(sourceLine[counter],500)\n        rougel.update(([sequence.split()], [targetLine[counter].split()]))\n        a=rougel.compute()\n        y=[targetLine[counter]]\n        blue.update(([sequence.split()],[[_y.split() for _y in y]]))\n        finalBlue=finalBlue+blue.compute()\n        \n        for keys, value in a.items():\n            final[keys]+=a[keys]\n        \n        length+=1\n    \n    fileS.close()\n    fileT.close()\n    return finalBlue,length\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T19:28:18.766871Z\",\"iopub.execute_input\":\"2022-11-16T19:28:18.767494Z\",\"iopub.status.idle\":\"2022-11-16T20:17:31.062057Z\",\"shell.execute_reply.started\":\"2022-11-16T19:28:18.767453Z\",\"shell.execute_reply\":\"2022-11-16T20:17:31.060934Z\"}}\nfinalBlue,length=evaluateSentence()\nprint(final)#estimated time for all test samples\nprint(\"Blue score: \",finalBlue)\nprint(length)\n\n# %% [markdown]\n# **Interactive Console**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-16T21:13:07.716210Z\",\"iopub.execute_input\":\"2022-11-16T21:13:07.716594Z\",\"iopub.status.idle\":\"2022-11-16T21:13:49.942468Z\",\"shell.execute_reply.started\":\"2022-11-16T21:13:07.716551Z\",\"shell.execute_reply\":\"2022-11-16T21:13:49.941341Z\"}}\nsequence = input() \nprint(inference(sequence))\n\n# %% [code]\n","metadata":{"_uuid":"d16fac99-4206-4c4c-b41d-1324695ba561","_cell_guid":"5b22fb93-b7bd-489c-a2a6-d73062071512","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}